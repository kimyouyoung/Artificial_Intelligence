{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그러나 너희는 택하신 족속이요 왕 같은 제사장들이요 거룩한 나라요 그의 소유가 된 백성이니 이는 너희를 어두운 데서 불러 내어 그의 기이한 빛에 들어가게 하신 이의 아름다운 덕을 선포하게 하려 하심이라 (벧전2:9)\n",
    "\n",
    "-------\n",
    "\n",
    "# Welcome to \"AI for All\" Final Exam (2020-2)\n",
    "\n",
    "Lecture Notes by idebtor@gmail.com, Handong Global University"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.4 선형 회귀를 위한 뉴론 만들기\n",
    "\n",
    "## Exercise - Final Exam (3 Problems)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 에폭$^{epoch}$에 따른 오차제곱합 SSE 의 변화를 관찰하기\n",
    "\n",
    "앞에서 구현한 클래스 Neuron에  오차제곱합 SSE(Sum of Squared Error)를 추적할 수 있도록 인터턴스 변수를 추가하고 당뇨병 자료로 에폭에 따른 오차제곱합 SSE의 변화를 시각화 하십시오. \n",
    "\n",
    "- 오차제곱합 인스턴스 변수 sse는 `__init__` 메소드에 추가하고 초기화하는 것보다 fit() 메소드가 낫습니다. 왜냐하면, sse 배열의 크기는 epoch에 따라 결정되기 때문입니다. \n",
    "\n",
    "#### Solution Part 1:   오차제곱합 sse 인스턴스 변수와 sse 계산하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neuron:\n",
    "    def __init__(self):\n",
    "        self.w = 1.0\n",
    "        self.b = 1.0\n",
    "\n",
    "    def forpass(self, x):  \n",
    "        y_hat = x * self.w + self.b \n",
    "        return y_hat\n",
    "\n",
    "    def backprop(self, x, error): \n",
    "        w_grad = x * error\n",
    "        b_grad = 1 * error\n",
    "        return w_grad, b_grad\n",
    "\n",
    "    def fit(self, x, y, epochs = 100): \n",
    "        self.sse =  np.zeros(epochs)                       # initialize sse array as 0\n",
    "        \n",
    "        for i in range(epochs):                            # epoch만큼 반복합니다.\n",
    "            sse_i = 0\n",
    "            for x_i, y_i in zip(x, y):                     # 모든 샘플에 대해 반복합니다.\n",
    "                y_hat = self.forpass(x_i)                   # 순전파 계산\n",
    "                error = (y_i - y_hat)                       # 오차 계산\n",
    "                w_grad, b_grad = self.backprop(x_i, error)  # 역방향 계산\n",
    "                self.w += w_grad                            # 가중치 조정\n",
    "                self.b += b_grad                            # 편향 조정\n",
    "                sse_i += (error**2)\n",
    "            self.sse[i] = sse_i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solution Part 2: 학습과 모델 결과 출력하기\n",
    "\n",
    "- Diabetes 자료를 가져오기\n",
    "- Neuron클래스 객체인 neuron을 생성하기 \n",
    "- neuron객체의 fit() 메소드로 모델 생성하기\n",
    "- neuron.sse로 평균제곱오차(mse) 계산하기\n",
    "- 모델의 w와 b, 또한 mse를 출력하여 예전의 답과 같은지 확인하기\n",
    "\n",
    "```\n",
    "Weight: 913.5656499923714\n",
    "  Bias: 123.39181064719298\n",
    "   MSE: [9005.36195584 7469.28242258 7259.87058017 7227.18887467 7220.915469\n",
    " 7219.36559842 7218.89987889 7218.74451808 7218.69033115 7218.67110647]\n",
    " ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_diabetes\n",
    "diabetes = load_diabetes()\n",
    "\n",
    "x = diabetes.data[:, 2]           # bmi feature 읽어오기 \n",
    "y = diabetes.target               # target/label 읽어오기\n",
    "epochs = 10\n",
    "\n",
    "neuron = Neuron()    # create a neuron\n",
    "neuron.fit(x, y, epochs)    # training with epochs = 10 \n",
    "mse = neuron.sse / len(x)                 # get mse from sse\n",
    "print(\"Weight:\", neuron.w)\n",
    "print(\"  Bias:\", neuron.b)\n",
    "print(\"   MSE:\", mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solution Part 3: MSE (평균 제곱 오차) 시각화 하기\n",
    "\n",
    "<img src=\"https://github.com/idebtor/KMOOC-ML/blob/master/ipynb/images/ai4all-diabetes11.png?raw=true\" width=\"400\">\n",
    "<center>그림 2: 경사하강법 오차(MSE)의 변화 </center>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x0 = np.arange(epochs)\n",
    "plt.plot(x0, mse)            \n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('mse')\n",
    "plt.title('Gradient Descent MSE for Diabetes Dataset')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 제 1 강 신경망을 내 손으로 만져보기(Gate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise - Final Exam (4 Problems)\n",
    "\n",
    "#### AND, NAND, OR, NOR 게이트 로직을 다룰 수 있는 Gate라는 클래스를 만들고 출력을 확인하십시오. \n",
    "\n",
    "<img src=\"https://github.com/idebtor/KMOOC-ML/blob/master/ipynb/images/truthtable.png?raw=true\" width=\"400\">\n",
    "<center>그림 1:  OR, AND, NAND, XOR 진리표 </center>\n",
    "\n",
    "#### Part 1:  Gate 클래스 정의하기 (sse 인스턴스 변수와 코드 포함)\n",
    "\n",
    "#### Solution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Gate:\n",
    "    def __init__(self):\n",
    "        self.w = np.array([1.0, 1.0])         \n",
    "        self.b = np.array([1.0]) \n",
    "        self.eta = 0.1\n",
    "\n",
    "    def forpass(self, x):\n",
    "        z = np.sum(x * self.w) + self.b\n",
    "        a = self.activation(z)\n",
    "        return a\n",
    "\n",
    "    def backprop(self, x, error): \n",
    "        w_grad = self.eta * error * x\n",
    "        b_grad = self.eta * error * 1\n",
    "        return w_grad, b_grad\n",
    "    \n",
    "    def activation(self, z): \n",
    "        a = 1/(1 + np.exp(-z))\n",
    "        return a\n",
    "\n",
    "    def fit(self, x, y, epochs = 1000):                    # default epochs = 1000\n",
    "        self.sse = np.zeros(epochs)\n",
    "        \n",
    "        for i in range(epochs):                            # epoch만큼 반복합니다. \n",
    "            sse_i = 0\n",
    "            for x_i, y_i in zip(x, y):                     # 모든 샘플에 대해 반복합니다.\n",
    "                y_hat = self.forpass(x_i)                   # 순전파 계산\n",
    "                error = (y_i - y_hat)                       # 오차 계산\n",
    "                w_grad, b_grad = self.backprop(x_i, error)  # 역방향 계산\n",
    "                self.w += w_grad                            # 가중치 조정\n",
    "                self.b += b_grad                            # 편향 조정\n",
    "                sse_i += (error**2)\n",
    "            self.sse[i] = sse_i\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 입력 x와 y_and, y_nand, y_or, y_xor 레이블(타깃)을 정의하기 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([[0, 0], [0, 1], [1, 0], [1, 1]]) \n",
    "y_and  = np.array([[0], [0], [0], [1]])\n",
    "y_nand = np.array([[1], [1], [1], [0]])\n",
    "y_or   = np.array([[0], [1], [1], [1]])\n",
    "y_nor  = np.array([[1], [0], [0], [0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 2: gate_and 객체와 훈련하기  (epochs = 500)\n",
    "\n",
    "- gate_and 객체를 생성하기\n",
    "- 훈련하기\n",
    "- mse 구하기 \n",
    "- 모델의 결과와 예측을 출력하기 \n",
    "\n",
    "```\n",
    "AND Gate\n",
    "Weight: [4.31247748 4.30140545]\n",
    "  Bias: [-6.6310874]\n",
    "[0 0] [0] y_hat:[0.00131699]\n",
    "[0 1] [0] y_hat:[0.08869437]\n",
    "[1 0] [0] y_hat:[0.08959338]\n",
    "[1 1] [1] y_hat:[0.87897885]\n",
    "```\n",
    "\n",
    "#### Solution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 500\n",
    "gate_and  = Gate()\n",
    "gate_and.fit(x, y_and, epochs)\n",
    "mse = gate_and.sse / len(x)          # get mse from sse\n",
    "\n",
    "print(\"AND Gate\")\n",
    "print(\"Weight:\", gate_and.w)\n",
    "print(\"  Bias:\", gate_and.b)\n",
    "for i in range(len(x)): \n",
    "    print('{} {} y_hat:{}'.format(x[i], y_and[i], gate_and.activation(np.sum(x[i] * gate_and.w) + gate_and.b)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 3: epoch에 따른 mse 시각화 하기\n",
    "\n",
    "<img src=\"https://github.com/idebtor/KMOOC-ML/blob/master/ipynb/images/ai4all-gate-mse.png?raw=true\" width=\"400\">\n",
    "<center>그림 4: Gradient Descent MSE for Logic Gate</center>\n",
    "\n",
    "#### Solution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(np.arange(epochs), mse)\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('mse')\n",
    "plt.title('Gradient Descent MSE for Logic Gate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 4: NAND Gate\n",
    "\n",
    "- gate_nand 객체를 생성하기\n",
    "- 훈련하기\n",
    "- mse 구하기 \n",
    "- 모델의 결과와 예측을 출력하기 \n",
    "\n",
    "```\n",
    "NAND Gate\n",
    "Weight: [-4.25914936 -4.24781832]\n",
    "  Bias: [6.55111244]\n",
    "[0 0] [1] y_hat:[0.99857351]\n",
    "[0 1] [1] y_hat:[0.90914949]\n",
    "[1 0] [1] y_hat:[0.90820923]\n",
    "[1 1] [0] y_hat:[0.12391631]\n",
    "```\n",
    "\n",
    "#### Solution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gate_nand = Gate()\n",
    "gate_nand.fit(x, y_nand, epochs)\n",
    "mse = gate_nand.sse / len(x)\n",
    "\n",
    "print(\"NAND Gate\")\n",
    "print(\"Weight:\", gate_nand.w)\n",
    "print(\"  Bias:\", gate_nand.b)\n",
    "for i in range(len(x)): \n",
    "    print('{} {} y_hat:{}'.format(x[i], y_nand[i], gate_nand.activation(np.sum(x[i] * gate_nand.w) + gate_nand.b)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 4: OR Gate\n",
    "\n",
    "- gate_or 객체를 생성하기\n",
    "- 훈련하기\n",
    "- mse 오차 구하기 \n",
    "- 모델의 결과와 예측을 출력하기 \n",
    "\n",
    "```\n",
    "OR Gate\n",
    "Weight: [5.42570445 5.43193129]\n",
    "  Bias: [-2.21461759]\n",
    "[0 0] [0] y_hat:[0.09844548]\n",
    "[0 1] [1] y_hat:[0.96148065]\n",
    "[1 0] [1] y_hat:[0.96124937]\n",
    "[1 1] [1] y_hat:[0.99982368]\n",
    "```\n",
    "#### Solution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gate_or = Gate()\n",
    "gate_or.fit(x, y_or, epochs)\n",
    "mse = gate_or.sse / len(x)\n",
    "\n",
    "print(\"OR Gate\")\n",
    "print(\"Weight:\", gate_or.w)\n",
    "print(\"  Bias:\", gate_or.b)\n",
    "for i in range(len(x)): \n",
    "    print('{} {} y_hat:{}'.format(x[i], y_or[i], gate_or.activation(np.sum(x[i] * gate_or.w) + gate_or.b)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 4: NOR gate\n",
    "\n",
    "- gate_nor 객체를 생성하기\n",
    "- 훈련하기\n",
    "- mse 구하기 \n",
    "- 모델의 결과와 예측을 출력하기 \n",
    "\n",
    "```\n",
    "NOR Gate\n",
    "Weight: [-5.33364922 -5.34053195]\n",
    "  Bias: [2.16668323]\n",
    "[0 0] [1] y_hat:[0.8972175]\n",
    "[0 1] [0] y_hat:[0.04016179]\n",
    "[1 0] [0] y_hat:[0.04042795]\n",
    "[1 1] [0] y_hat:[0.00020191]\n",
    "```\n",
    "\n",
    "#### Solution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gate_nor = Gate()\n",
    "gate_nor.fit(x, y_nor, epochs)\n",
    "mse = gate_nor.sse / len(x)\n",
    "\n",
    "print(\"NOR Gate\")\n",
    "print(\"Weight:\", gate_nor.w)\n",
    "print(\"  Bias:\", gate_nor.b)\n",
    "for i in range(len(x)): \n",
    "    print('{} {} y_hat:{}'.format(x[i], y_nor[i], gate_nor.activation(np.sum(x[i] * gate_nor.w) + gate_nor.b)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 제 3 강 신경망을 내 손으로 만져보기(tf.keras & MNIST)\n",
    "\n",
    "---------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 참고자료: np.argmax() 사용하기 \n",
    "\n",
    "넘파이의 argmax()를 사용하여 배열 요소 중 최대값의 인덱스 구할 수 있습니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "x = np.array([\n",
    "    [0.1, 0.9, 0.2],\n",
    "    [0.3, 0.1, 0.7],\n",
    "    [0.2, 0.6, 0.3],\n",
    "    [0.9, 0.1, 0.1]\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.argmax(x))                  # output: 1\n",
    "print(np.argmax(x, axis=0))          # output: [3 0 1]\n",
    "print(np.argmax(x, axis=1))          # output: [1 2 1 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "넘파이 배열 x는 형상(크기) 4x3 입니다. 여기서 4는 axis 0의 크기이며, 3은 axis 1의 크기임을 기억하십시오. \n",
    "\n",
    "- np.argmax(x)는 x 배열을 1차원으로 평면화된 배열에 대해 최대값을 갖는 인덱스를 반환하고, \n",
    "- np.argmax(x, axis=0)는 첫번째 축인 row 방향(세로방향)으로 구성되는 요소 중 최대인 인덱스를 반환하며,  \n",
    "- np.argmax(x, axis=1)는 두번째 축인 column 방향(가로방향)으로 구성되는 요소 중 최대인 인덱스를 반환합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise - Final Exam (4 Problems)\n",
    "\n",
    "\"신경망을 내 손으로 만져보기(tf.keras & MNIST)\"의 __Step 6. 분석과정__에서 우리는 다음과 같이 모델 model객체를 생성하였습니다. 이와 관련하여 다음 질문들에 대하여 답을 하십시오.  \n",
    "\n",
    "#### Step 6. 분석 과정: Postprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "############# set the seed value to get repeatable random sequences ############\n",
    "seed_value = 0\n",
    "import random\n",
    "import os\n",
    "os.environ['PYTHONHASHSEED']=str(seed_value)                  \n",
    "tf.random.set_seed(seed_value)\n",
    "np.random.seed(seed_value)\n",
    "random.seed(seed_value)\n",
    "################################################################################\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "x_train, x_test = x_train/255.0, x_test/255.0\n",
    "print(x_train.shape)\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28, 28)), \n",
    "    tf.keras.layers.Dense(256, activation='relu'),      #은닉층\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(10, activation='softmax')     #출력층\n",
    "])\n",
    "model.compile(\n",
    "    optimizer = 'adam',\n",
    "    loss='sparse_categorical_crossentropy', \n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "history = model.fit(x_train, y_train, epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 예측 정확도 계산하기\n",
    "\n",
    "\"신경망을 내 손으로 만져보기(tf.keras & MNIST)\"의 __Step 6. 분석과정__에서 생성한 모델 `model`객체로 테스트 데이터셋으로 예측 정확도를 계산하십시오.  무작위 초기화로 말미암아 훈련 모델이 서로 다를 수 있으므로, 예측 정확도에 다소 차이가 있을 수 있습니다. \n",
    "\n",
    "```\n",
    " Accuracy: 0.9796\n",
    "```\n",
    "#### Solution 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat = model.predict(x_test)\n",
    "\n",
    "correct = 0\n",
    "for i, iyhat in enumerate(yhat):\n",
    "    predicted = np.argmax(iyhat)\n",
    "    if predicted == y_test[i]:\n",
    "        correct += 1\n",
    "\n",
    "print(\" Accuracy:\", correct/len(yhat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pythonic Coding \n",
    "\n",
    "np.argmax()의 axis 매개변수를 사용하면 위의 코드를 두줄로 처리할 수 있습니다. np.sum() 혹은 np.mean()도 활용하십시오. \n",
    "\n",
    "#### Solution 2: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = np.sum(y_test == np.argmax(yhat, axis=1))\n",
    "print(\" Accuracy:\", predicted/len(yhat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 2. 예측 오류 샘플 분석하기 \n",
    "\n",
    "예측 오류를 분석하기 위하여, 예측이 잘못된 샘플들을 몇 개나 되는지 찾고, 정확도를 계산하십시오.  무작위 초기화로 말미암아 훈련 모델이 서로 다를 수 있습니다. 다음은 정확도와 잘못 예측한 샘플들 처음 몇 개를 예시로 보여주는 것입니다. \n",
    "\n",
    "```\n",
    "Accuracy: 0.9796\n",
    "  Samples: [115, 149, 151, 247, 321, 340, 381, 445, 495, 530]\n",
    "   Labels: [4, 2, 9, 4, 2, 5, 3, 6, 8, 9]\n",
    "Predicted: [9, 4, 8, 6, 7, 3, 7, 0, 2, 4]\n",
    "```\n",
    "#### Solution: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mindex = [] # list of index predicted wrong \n",
    "mlabel = [] # list of missed labels \n",
    "missed = [] # list of predicted wrong\n",
    "\n",
    "for i, iyhat in enumerate(yhat):\n",
    "    predicted = np.argmax(iyhat)\n",
    "    if predicted != y_test[i]:\n",
    "        mindex.append(i)\n",
    "        mlabel.append(y_test[i])\n",
    "        missed.append(predicted)\n",
    "        \n",
    "        \n",
    "print(\" Accuracy:\", 1 - len(missed)/len(yhat))\n",
    "print(\"  Samples:\", mindex[:10])\n",
    "print(\"   Labels:\", mlabel[:10])\n",
    "print(\"Predicted:\", missed[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3: 예측 오류 틀린 샘플들 찾아내기 \n",
    "\n",
    "예측 오류 결과를 출력할 때, 몇 번째의 샘플이며, 어떤 숫자를 어떻게 다르게 인식했는지 즉 레이블(label)과 예측값(predicted)에 대한 리스트를 작성하고, 리스트의 처음 열개를 아래와 같은 형식으로 출력하십시오. _아래의 결과 사례는 학습 모델에 따라 다를 수도 있습니다._ \n",
    "\n",
    "```\n",
    "[(115, 4, 9), (149, 2, 4), (151, 9, 8), (247, 4, 6), (321, 2, 7), (340, 5, 3), (381, 3, 7), (445, 6, 0), (495, 8, 2), (530, 9, 4)]\n",
    " ```\n",
    " \n",
    " #### Solution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missed = []\n",
    "for i, iyhat in enumerate(yhat):\n",
    "    predicted = np.argmax(iyhat)\n",
    "    if predicted != y_test[i]:\n",
    "        missed.append((i, y_test[i], predicted))\n",
    "\n",
    "print(missed[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4: 예측 오류 샘플들을 시각화하기 \n",
    "\n",
    "예측 오류가 발생한 처음 열개의 샘플 이미지를 다음과 같은 형식으로 시각화하고, 레이블 및 예측값을 이미지 바로 밑에 표시하십시오. 이 결과는 학습 모델의 결과와 다를 수도 있습니다. \n",
    "\n",
    "<img src=\"https://github.com/idebtor/KMOOC-ML/blob/master/ipynb/images/ai4all-predicted10.png?raw=true\" width=\"800\">\n",
    "<center>그림 4: 예측 오류가 발생한 첫 10장의 이미지</center>\n",
    "\n",
    "#### Hint: 샘플 하나를 그리는 코드\n",
    "```\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.title(\"Sample:\" + str(missed[0][0]))\n",
    "plt.xlabel(\"label:\" + str(missed[0][1]) + \", predicted:\" + str(missed[0][2]))\n",
    "plt.imshow(x_train[missed[0][0]], cmap='gray')  \n",
    "plt.show()\n",
    "```\n",
    "#### Solution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,20))\n",
    "for i in range(len(missed[:10])):\n",
    "    plt.subplot(10, 10, i + 1)\n",
    "    plt.xticks([]) \n",
    "    plt.yticks([])\n",
    "    plt.title(\"Sample: \" + str(missed[i][0]))\n",
    "    plt.xlabel(\"label:\" + str(missed[i][1]) + \", predicted:\" + str(missed[i][2]))\n",
    "    plt.imshow(x_train[missed[i][0]], cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------\n",
    "__Be joyful always!__ 1 Thes.5:16"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
