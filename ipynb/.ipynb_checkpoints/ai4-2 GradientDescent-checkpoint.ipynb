{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그러나 너희는 택하신 족속이요 왕 같은 제사장들이요 거룩한 나라요 그의 소유가 된 백성이니 이는 너희를 어두운 데서 불러 내어 그의 기이한 빛에 들어가게 하신 이의 아름다운 덕을 선포하게 하려 하심이라 (벧전2:9)\n",
    "\n",
    "-------\n",
    "\n",
    "# Welcome to \"AI for All\"\n",
    "\n",
    "Lecture Notes by idebtor@gmail.com, Handong Global University"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Chapter 4. 경사하강법(Gradient Descent)\n",
    "\n",
    ":본 단원은 참고문헌 (3) & (4)에서 대부분  발췌한 것입니다. \n",
    "\n",
    "--------------\n",
    "경사하강법으로 학습하여 모델을 구하는 방법을 알아봅니다. \n",
    "\n",
    "###  Review: Seaborn pairplot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 그래프로 경사하강법의 의미를 알아보기\n",
    "\n",
    "앞에서 우리는 당뇨병 데이터셋에서 세 번째 특성과 타깃 값을 가지고 산점도를 그려 보았는데, 이 산점도를 대표하여 표현할 수 있는 직선을 그린다면 어떤 모양일까요?  다음 그림들 중에 하나를 선택해 보십시오. 어떤 직선이 데이터를 가장 잘 표현하고 있나요?\n",
    "\n",
    "<table attr=\"center\"><tr>\n",
    "<td><img src=\"https://github.com/idebtor/KMOOC-ML/blob/master/ipynb/images/ai4all-diabetes3.png?raw=true\", width=250></td> \n",
    "<td><img src=\"https://github.com/idebtor/KMOOC-ML/blob/master/ipynb/images/ai4all-diabetes4.png?raw=true\", width=250/></td> \n",
    "<td><img src=\"https://github.com/idebtor/KMOOC-ML/blob/master/ipynb/images/ai4all-diabetes5.png?raw=true\", width=250/></td> \n",
    "</tr></table>\n",
    "<div style='text-align:center'>그림 1: 당뇨병 데이터셋(세 번째 특성) 산점도와 선형 회귀</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "가운데 그림의 직선이 다른 두 그림의 직선들보다 더 잘 데이터셋을 표현한다는 것을 직감적으로 알 수 있습니다. 데이터들의 중심을 지나는 직선이 답이라는 것을 쉽게 알 수 있지만, 이 그래프는 10개의 특성들 중에 하나와 타깃(레이블)과의 산점도라는 것입니다. 만약 10개의 특성으로 그래프를 그린다면 어떻게 표현할 수 있을까요? 11차원의 그래프는 우리가 그릴 수도 없고, 그러한 공간을 상상할 수도 없습니다. 이렇게 상상할 수도 없는 다차원의 공간을 <span style=\"color:red\"> 초평면(hyperplane)</span>이라고 부릅니다. \n",
    "\n",
    "그래서 대개 여러 개의 특성을 가진 데이터에서 특성의 개수를 한 두 개만 사용하여 2차원 혹은 3차원 그래프로 그려봅니다. 그러면, 제한적이긴 하지만, 알고리즘에 대한 직관을 쉽게 얻을 수 있게 됩니다. 항상 그런 것은 아니지만, 낮은 차원에서 얻은 직관은 높은 차원으로 확장될 수 있으므로, 위와 같이 입력 데이터의 특성을 한 개 골라 시각화하는 경우가 많습니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 선형 회귀와 경사 하강법의 관계를 이해하기\n",
    "회귀 알고리즘의 목표는 산점도 그래프를 잘 표현하는 직선의 식을 찾는 것이었습니다. 즉 입력 데이터와 타깃 데이터를 통해 직선의 식을 찾는다는 것은 곧 기울기와 절편의 값을 찾는 것이었습니다. \n",
    "\n",
    "__경사 하강법(gradient descent)__이 바로 기울기와 절편의 값을 찾아가는 방법 중에 하나입니다. 경사 하강법은 모델이 데이터를 잘 표현할 수 있도록 기울기(변화율)를 사용하 모델을 조금씩 조정하는 최적화 알고리즘입니다. 경사 하강법이라는 최적화 알고리즘을 파이썬으로 이제 구현하려고 합니다. 그 전에 몇 가지 사전 지식이 필요합니다. 이 부분을 잘 이해하는 것이 중요합니다. 일단 이해하고 나면, 복잡한 계산은 컴퓨터가 알아서 해줄 것입니다. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 예측값 $\\hat{y}$ 알아보기 \n",
    "\n",
    "기계학습과 딥러닝 분야에서는 기울기 $a$를 종종 __가중치(weight)__를 의미하는 $w$나 계수를 의미하는 $\\theta$로 표기합니다. 그리고, $y$는 $\\hat{y}$로 표기하며 읽을 때는 와이-햇(y-hat)이라고 읽습니다. 절편(intercept)은 __편향(bias)__이라고 부릅니다. 즉, 앞으로는 $y = ax + b$로 표현했던 모델을 $\\hat{y} = wx + b$로 표현할 것입니다. \n",
    "\n",
    "여기서 가중치 $w$와 편향 $b$는 알고리즘이 찾는 규칙을 의미하고, $\\hat{y}$는 모델이 예측한 값(예측값, 출력)을 의미합니다. 가중치에 대해서는 나중에 더 깊이 알아보고, 지금은 예측값과 변화율에 집중하여 살펴보겠습니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 예측값이란 무엇일까요? \n",
    "\n",
    "우리가 입력과 출력 데이터$(x, y)$를 통해 규칙$(a, b)$을 발견하면 모델을 만들었다고 했습니다. 그 모델에 새로운 입력값을 넣으면, 어떤 출력이 나오는데, 이 값이 모델을 통해 예측한 값입니다. \n",
    "\n",
    "예를 들어 $y = 3x + 2$라는 모델의 $x$에 5를 넣으면 17이라는 값이 나오는데, 이 값이 모델을 통하여 예측한 값입니다. 그래서 타깃 데이터를 표현하는 $y$라는 문자와 구분하기 위해 $\\hat{y}$이라는 문자를 따로 준비한 것입니다. 그러니까 $y$와 $\\hat{y}$은 어떤 결과라는 점은 동일합니다. $\\hat{y}$을 적용하여 모델에 관한 식을 다시 쓰면 다음과 같습니다. \n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{y} = wx + b \\tag{1}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 예측값의 의미를 이해했으니 예측값으로 어떻게 모델을 조정할 수 있는 알아보겠습니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 예측값으로 모델 찾아가기\n",
    "\n",
    "위의 식(1)에서 우리가 찾고 싶은 것은 훈련 데이터 $(x, y)$를 잘 나타낼 수 있는 $w$와 $b$입니다. $W$와 $b$를 찾는 알고리즘은 다음과 같습니다. \n",
    "\n",
    "> __훈련 데이터에 적합한 가중치 $w$와 편향 $b$를 구하는 방법__\n",
    "> 1. 무작위로 w와 b를 초기화한다. (무작위로 모델 만들기)\n",
    "> 1. $x$에서 샘플 하나를 선택하여 $\\hat{y}$을 계산한다. (무작위로 모델 예측하기)\n",
    "> 1. $\\hat{y}$과 선택한 샘플의 타깃(레이블) $y$를 비교한다. (예측한 값과 실제 값과 비교하기)\n",
    "> 1. $\\hat{y}$이 $y$와 더 가까워지도록 $w, b$를 조정한다. (모델 조정하기)\n",
    "> 1. 모든 샘플을 처리할 때까지 다시 (2) ~ (4) 항목을 반복한다. \n",
    ">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위의 방법은 아주 직관적입니다. \n",
    "\n",
    "그런데, 항목(4) __$\\hat{y}$이 $y$와 더 가까워지도록 $w, b$를 조정한다__는 어떤 의미일까요?  일차함수와 같은 단순한 모델에서는 $w$와 $b$의 부호를 보고 $\\hat{y}$이 커지거나 작아지는 방향을 짐작하기 쉽겠지만, 모델이 복잡해질수록 체계적인 방법이 필요합니다. 일단 위의 방법대로 모델을 한 번 조정해 보면서 어떤 체계적인 방법이 필요한지 알아보도록 합니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 훈련하기 준비 작업\n",
    "앞 단원에서 미리 맛보기를 한 당뇨병 데이터셋을 다시 불러와서 그 중에 __세 번째 특성과 타깃(레이블)__ 데이터를 각각 $x, y$ 변수에 저장하여 작업을 준비합니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_diabetes\n",
    "diabetes = load_diabetes()\n",
    "\n",
    "x = diabetes.data[:, 2]                    # bmi feature 읽어오기 \n",
    "y = diabetes.target                    # target/label 읽어오기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "변수 x, y의 형상이 각각 다음과 같은 것을 확인하십시오. \n",
    "```\n",
    "(442,)\n",
    "(442,)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x.shape)\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 훈련 데이터에 맞는 $w$와 $b$찾아보기\n",
    "\n",
    "$w$와 $b$가 바뀌었을 때, $\\hat{y}$이 어떻게 변하는지 알아내는 가장 간단한 방법은 실제로 계산을 해보는 것입니다. \n",
    "\n",
    "#### 1. $w$와 $b$ 초기화 하기 \n",
    "\n",
    " $w$와 $b$를 무작위로 초기화합시다. 여기에서는 두 값을 모두 실수 1.0으로 정합니다. 쉽게 말해, 아직  $w$와 $b$를 어떻게 초기화할지 정하지 않았으므로, 임시적으로 1.0이라고 정한 것입니다. 물론, 이 방법은 $w$의 개수가 많아지면 번거롭기도 하고 바람직한 방법도 아닙니다. \n",
    " \n",
    "지금은 계산하기 쉬운 값 1.0으로 직접 정한 것입니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = 1.0\n",
    "b = 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. 훈련 데이터의 첫 번째 샘플 데이터로 $\\hat{y}$ 구하기\n",
    "\n",
    "이제 임시로 만들 모델로 훈련 데이터의 첫 번째 샘플 `x[0]`에 대한 $\\hat{y}$을 계산해 보겠습니다. 계산한 $\\hat{y}$은 변수 `y_hat`변수에 저장합니다. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = x[0] * w + b\n",
    "print(y_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. 타깃과 예측값 비교하기\n",
    "에측값 `y_hat`을 구했으니, 이제 타깃값(레이블, 실제값)을 출력하여 비교합니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x[:5])\n",
    "print(y[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. $w$ 값을 조정해 예측값 바꾸기\n",
    "\n",
    "우리가 예측한 `y_hat`은 1.06... 인데, 타깃은 151.0입니다. 차이가 많이 납니다. `w`와 `b`를 무작위 값으로 정했기 때문에 예측 결과가 잘 나오지 않은 것이 당연합니다.  이제 `w`와 `b`를 좀 더 바람직한 방향으로 바꾸어 보려고 합니다. 어떤 방향으로 바꾸면 `y_hat`이 `y[0]`에 가까워질 수 있을까요? 가장 쉬운 방법은 `w`와 `b`를 조금씩 변경해서 `y_hat`이 증가하는지 또는 감소하는지 살펴보는 것입니다.  \n",
    "\n",
    "먼저 `w`를 0.1만큼 증가시키고 `y_hat`의 변화량을 관찰해 보십시오. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_inc = w + 0.1\n",
    "y_hat_inc = x[0] * w_inc + b\n",
    "print(y_hat_inc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`w`값을 0.1만큼 증가시킨 후에 $\\hat{y}$을 다시 계산하여 `y_hat_inc`를 출력해보니 1.067...이므로, 그 전에 구한 `y_hat` 1.061...보다 조금 증가했습니다. \n",
    "\n",
    "#### 5. $w$값 조정한 후, 예측값 증가 정도 확인하기 \n",
    "\n",
    "그러면,`w`가 0.1만큼 증가했을 때, `y_hat`이 얼마나 증가했는지 계산해 보겠습니다. `y_hat`이 증가한 양을 `w`가 증가한 양으로 나누면 됩니다. 이것을 코딩으료 표현하면 다음과 같습니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_rate = (y_hat_inc - y_hat) / (w_inc - w)\n",
    "print(w_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이렇게 계산한 값(0.616...)을 첫 번째 훈련 데이터 `x[0]`에 대한 __`w`의 변화율__이라고 합니다. \n",
    "\n",
    "<span style=\"color:blue\"> 그런데, 신기하게도 `w_rate`에 대한 코드를 수식으로 적어서 정리하면, 변화율은 결국 훈련 데이터의 첫 번째 샘플인 `x[0]`이라는 것을 알 수 있습니다.</span>\n",
    "\n",
    "\\begin{align}\n",
    "w_{rate} &= \\frac{\\hat{y}_{inc} - \\hat{y}}{w_{inc} - w} \\\\\n",
    "        &= \\frac{(x[0] * w_{inc} + b) - (x[0] * w + b)}{w_{inc} - w}  \\\\\n",
    "        &= \\frac{x[0] * (w_{inc} - w)}{w_{inc}  - w} = x[0]\n",
    "\\end{align}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "지금까지 계산한 것을 정리합니다. \n",
    "\n",
    "`y_hat`의 값은 타깃값 `y`보다 작으므로, `y_hat`을 증가시켜야 합니다. 이때 변화율은 양수이므로, `w`값을 증가시키면 `y_hat`값을 증가시킬 수 있습니다. (그래서, 우리는 0.1 정도 증가시켜서 계산해보았습니다) \n",
    "\n",
    "만약, 변화율이 음수일 때, `y_hat`을 증가시켜야 한다면 어떻게 해야 할까요? `w`값을 감소시키면 됩니다. 하지만, 이 방법은 변화율이 양수일 때와 음수일 때를 구분해야 하므로 번거롭습니다. \n",
    "\n",
    "다음 내용을 읽고 나면, 이 문제를 효율적으로 해결하는 방법이 바로 __변화율__ 그 자체라는 것을 알 수 있을 것입니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 변화율로 가중치 조정하기 \n",
    "\n",
    "지금부터 `w`와 `b`를 변화율로 조정하는 방법을 알아봅니다. 먼저 `w`의 조정 방법을 알아보겠습니다. \n",
    "\n",
    "#### 1. 변화율이 양수일 때 가중치 조정하기\n",
    "다음 그래프로는 변화율이 0보다 큰 경우를 가정한 그래프입니다. \n",
    "\n",
    "<img src=\"https://github.com/idebtor/KMOOC-ML/blob/master/ipynb/images/ai4all-diabetes6.png?raw=true\" width=\"300\">\n",
    "<center>그림 2: w가 증가하면 y_hat이 증가하는 경우(변화율이 양수일 때) </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "앞에서 `y_hat`의 값은 타깃`y`보다 훨씬 작은 값이었습니다. 즉, `y_hat`이 더 많이 증가해야 하는 상황입니다. 이 상황에서 `w`가 증가하면 `y_hat`도 증가합니다. 이때 변화율이 양수인 점을 이용하여 __변화율을 `w`에 더하는 방법__으로 `w`를 증가시킬 수도 있습니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. 변화율이 음수일 때 가중치 조정하기\n",
    "그러면 변화율이 0보다 작을 때, `y_hat`을 증가시키려면 어떻게 해야 할까요? 다음은 변화율이 0보다 작은 직선의 식을 그림으로 나타낸 것입니다. \n",
    "\n",
    "<img src=\"https://github.com/idebtor/KMOOC-ML/blob/master/ipynb/images/ai4all-diabetes7.png?raw=true\" width=\"300\">\n",
    "<center>그림 3: w가 감소하면 y_hat이 증가하는 경우(변화율이 음수일 때) </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 상황에서는 `w`가 증가하면, `y_hat`은 감소합니다. 반대로 말하면, `w`가 감소하면, `y_hat`은 증가합니다. 이때 변화율이 음수인 점을 이용하면, 앞에서 한 대로 변화율을 더하는 방법으로 `y_hat`의 값을 증가시킬 수 있습니다. \n",
    "\n",
    "두 경우 모두 `w`에 `w_rate`를 더하면 되는 것입니다! 계산이 아주 쉬워진 것이죠. __즉, 가중치 `w`를 조정하는 방법은 두 경우 모두 `w + w_rate`입니다.__  \n",
    "\n",
    "다음은 가중치를 조정하는 예입니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_new = w + w_rate\n",
    "print(w_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 변화율로 편향 $b$ 조정하기 \n",
    "\n",
    "이번에는 편향 $b$에 대한 변화율을 구한 다음, 변화율로 $b$를 조정하겠습니다. `b`를 0.1만큼 증가시킨 후, `y_hat`이 얼마나 증가했는지 계산하고 변화율도 계산합니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_inc = b + 0.1\n",
    "y_hat_inc = x[0] * w + b_inc\n",
    "print(y_hat_inc)\n",
    "\n",
    "b_rate = (y_hat_inc - y_hat) / (b_inc - b)\n",
    "print(b_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "변화율 b_rate을 보니 1입니다. 변화율이 1이라 함은 같이 움직인다는 것입니다. 편향이 1만큼 증가하면, `y_hat`도 1만큼 증가하고, 편향이 10만큼 증가하면, `y_hat`도 10만큼 증가한다는 것입니다. 이것은 수식으로도 증명이 됩니다. \n",
    "\n",
    "\\begin{align}\n",
    "b_{rate} &= \\frac{\\hat{y}_{inc} - \\hat{y}}{b_{inc} - b} \\\\\n",
    "        &= \\frac{(x[0] * w + b_{inc}) - (x[0] * w + b)}{b_{inc} - b}  \\\\\n",
    "        &= \\frac{b_{inc}  - b}{b_{inc}  - b} = 1\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\"> 결론적으로 `b`를 조정하기 위해서는 변화율이 1이므로 단순히 1을 더하면 됩니다. </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_new = b + 1\n",
    "print(b_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "지금까지 실습에서는 `y_hat`을 증가시켜야 하는 상황을 가정하고 `w`와 `b`를 조정하는 방법에 대해 알아보았습니다. 그런데, 이 방법은 수동적인 방법입니다. 그 이유는 이 방법이 다음과 같은 상황에 대해 적합하게 대처하지 못하기 때문입니다. \n",
    "\n",
    "> - `y_hat`이 타깃(레이블) `y`에 한참 미치지 못하는 값인 경우, `w`와 `b`를 더 큰 폭으로 수정할 수 없습니다. (앞에서 변화율만큼 수정을 했지만, 그 외에 더 이상할 수 업습니다)\n",
    "> - `y_hat`이 `y`보다 커지면 `y_hat`을 감소시키지 못합니다. \n",
    "\n",
    "어떻게 해야 이 문제를 해결할 수 있을까요?\n",
    "\n",
    "다음 실습에서는 `w`와 `b`를 좀 더 능동적으로 조정하는 방법인 오차 역전파(backpropagation)에 대해 알아 보겠습니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 오차 역전파로 가중치와 편향을 조정하기\n",
    "\n",
    "오차 역전파(backpropagation)는 $\\hat{y}$과 $y$의 차이를 이용하여 `w`와 `b`를 조정합니다. 오차 역전파라는 이름에서 알 수 있듯이 이 방법은 오차가 연이어 전파되는 모습으로 수행됩니다. 아쉽게도 여기서 다룰 예제는 아주 간단해서 오차가 연이어 전파되는 모습이 잘 보이지 않습니다. 나중에 조금 더 복잡한 예제를 다룰 때, 오차가 연이어 전파되는 모습이 나오면 한 번 다루도록 하겠습니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 가중치와 편형을 더욱 적절하게 조정하기\n",
    "\n",
    "앞의 예제에서는 __변화율__만으로 `w`와 `b`를 조정했습니다.  \n",
    "\n",
    "- 만약, $\\hat{y}$과 $y$의 차이가 크면 어떻게 해야 할까요? `w`와 `b`를 더 많이 증가시키면 되지 않을까요? \n",
    "\n",
    "- 또한 $\\hat{y}$이 $y$보다 커져서 $\\hat{y}$을 감소시켜야 한다면, 어떻게 해야 할까요? `w`와 `b`를 감소시켜야 하지 않을까요? \n",
    "\n",
    "- 이번에는 __$y$에서 $\\hat{y}$을 뺀 오차의 양을 변화율에 곱하는 방법으로__ `w`를 조정해 보겠습니다. 이렇게 하면  $\\hat{y}$이 $y$보다 많이 작은 경우, `w`와 `b`를 많이 바꿀 수 있습니다.  $\\hat{y}$이 $y$를 지나치면 `w`와 `b`의 방향도 바꿔 줍니다. 이것을 수식으로 표현하면 다음과 같습니다. \n",
    "\n",
    "```\n",
    "w_new = w + w_rate * (오차)\n",
    "b_new = b + 1 * (오차)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. 오차와 변화율을 곱하여 가중치 조정하기\n",
    "\n",
    "먼저 `x[0]`일 때, `w`의 변화율과 `b`의 변화율에 __오차를 곱한__ 다음 조정된 `w_new`와 `b_new`를 출력해 보겠습니다. 결과값을 보면 `w`와 `b`가 각각 큰 폭으로 바뀌었음을 알 수 있습니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error = y[0] - y_hat\n",
    "w_new = w + w_rate * error\n",
    "b_new = b + b_rate * error\n",
    "print(w_new, b_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. 두 번째 샘플 x[1]을 사용하기\n",
    "\n",
    "두 번째 샘플 `x[1]`을 사용하여 오차를 구하고 새로운 `w`와 `b`를 구해 보겠습니다. \n",
    "\n",
    "앞에서 `w_rate`식을 정리했을 때, 샘플값과 같아진다는 것을 알았으므로, 앞으로는 `w_rate`를 별도로 계산하지 않고, __샘플값을 그대로 사용합니다__. 즉, 여기서는 `x[1]`이 `w_rate`가 됩니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = x[1] * w_new + b_new\n",
    "error = y[1] - y_hat\n",
    "w_rate = x[1]\n",
    "w_new = w_new + x[1] * error\n",
    "b_new = b_new + 1 * error\n",
    "print(w_new, b_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`w`는 4만큼 커지고 `b`는 절반(약 150에서 75로)으로 줄어 들었습니다. 이런 방식으로 모든 샘플을 사용해 가중치와 편향을 조정합니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. 전체 샘플을 반복하기\n",
    "\n",
    "전체 샘플에 대해 앞의 과정을 반복하는 코드는 다음과 같습니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x_i, y_i in zip(x, y):\n",
    "    y_hat = x_i * w + b\n",
    "    error = y_i - y_hat\n",
    "    w = w + x_i * error\n",
    "    b = b + 1 * error\n",
    "print(w, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "파이썬의 zip()함수는 여러 개의 배열에서 동시에 요소를 하나씩 꺼내줍니다. 여기에서는 입력 x와 타깃 y 배열에서 요소를 하나씩 꺼내어 err를 계산하고 `w`와 `b`를 조정했습니다. 반복 구문 for를 사용하고 변수 이름만 달라졌을 뿐 이전과 동일합니다. \n",
    "\n",
    "#### 4. 그래프 그려보기\n",
    "지금까지 만든 모델이 전체 데이터셋을 잘 표현하는지 그래프를 그려 알아보겠습니다. 산점도 위에 `w`와 `b`를 사용한 직선을 그려보면 금방 알 수 있습니다. 직선 그래프를 그리려면 시작점과 종료점의 x, y좌료를 plot()함수에 전달하면 됩니다. x좌표 2개 `[-0.1, 0.15]` 를 지정하고, `y`좌표값은 `w`와 `b`를 사용해 계산하면 그래프를 그릴 수 있습니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.scatter(x, y)\n",
    "x0, x1 = -0.1, 0.18                 # set lower x, higher x in x-axis\n",
    "y0, y1 = x0 * w +b, x1 * w + b        \n",
    "plt.plot([x0, x1], [y0, y1], 'r')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "직선 모양이 어떤가요? 아주 만족스럽지는 않지만 산점도를 어느 정도 잘 표현한 것 같습니다. 어떻게 하면 조금 더 적합한 직선을 얻을 수 있을까요? 조금 더 반복해 볼까요? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. 여러 번 반복하기(epoch)\n",
    "\n",
    "대개 경사 하강법에서는 주어진 훈련 데이터로 학습을 여러 번 반복합니다. 이렇게 전체 훈련 데이터를 모두 이용하여 한 단위의 작업을 진행하는 것을 특별히 epoch(에폭)이라고 부릅니다. 일반적으로 수십에서 수천 번의 에폭을 반복합니다. 100번의 epoch를 실행하면서 직선이 어떻게 이동하는지 확인해 보겠습니다. 우리가 할 일은 앞의 for문 바깥쪽에 for문을 하나 더 추가하는 것이 전부입니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(100):\n",
    "    for x_i, y_i in zip(x, y):\n",
    "        y_hat = x_i * w + b\n",
    "        error = y_i - y_hat\n",
    "        w_rate = x_i\n",
    "        w = w + w_rate * error\n",
    "        b = b + 1 * error\n",
    "print(w, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "100번의 에폭를 반복하여 찾은 `w`와 `b`는 약 913.6, 123.4 정도입니다. `w`와 `b`를 이용하여 이 직선을 그래프로 나타내면 다음과 같습니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x, y, marker = '^', color = 'g')\n",
    "x0, x1 = -0.1, 0.18                 # set lower x, higher x in x-axis\n",
    "y0, y1 = x0 * w +b, x1 * w + b        \n",
    "plt.plot([x0, x1], [y0, y1], 'r')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "직선이 전체 데이터를 대표할 만한 모양세를 갖추고 있습니다. 드디어 이 데이터에 잘 맞는 기계학습 모델을 찾았습니다. \n",
    "\n",
    "\\begin{align}\n",
    "\\hat{y} = 913.6x + 123.4\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. 모델로 예측하기\n",
    "\n",
    "이번에는 입력 x에 없었던 새로운 데이터가 발생했다고 가정해 보겠습니다. 이 데이터에 대해 예측값을 얻으려면 어떻게 해야 할까요? 우리가 찾은 모델에 `x`를 넣고 계산하기만 하면 됩니다.  `x`가 0.18일 때, $\\hat{y}$의 값을 예측해보겠습니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_new = 0.18\n",
    "y_pred = x_new * w + b\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다음은 이 데이터를 산점도 위에 나타낸 것입니다. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x, y)\n",
    "plt.scatter(x_new, y_pred, color='red')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위 그래프에서 빨간 색으로 표시한 점이 바로 `x`가 0.18일 때 예측한 $\\hat{y}$으로 찍은 점입니다. 산점도의 추세로 보니 어느 정도 잘 예측한 것으로 같죠? 축하합니다! 기계학습 모델로 예측까지 수행했습니다. 바로 이것이 기계학습이 하는 일입니다. \n",
    "\n",
    "다음은 지금까지 실습한 과정을 간단히 요약한 것입니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "> __지금까지는 모델을 이렇게 만들었습니다__\n",
    "> 1. `w`와 `b`를 임의의 값(1.0, 1.0)으로 초기화하고, 훈련 데이터의 샘플을 하나씩 대입하여 $y$와 $\\hat{y}$의 오차를 구합니다. \n",
    "> 2. 앞 단계에서 구한 오차를 `w`와 `b`의 변화율에 곱하고 이 값을 이용하여 `w`와 `b`를 조정합니다. \n",
    "> 3. 만약 $\\hat{y}$이 $y$보다 커지면, 오차는 음수가 되어 자동으로 `w`와 `b`가 줄어들도록 조정이 됩니다. \n",
    "> 4. 반대로, $\\hat{y}$이 $y$보다 작으면, 오차는 양수가 되고,  `w`와 `b`가 커지도록 조정이 됩니다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise \n",
    "\n",
    "#### 모델로 예측하기\n",
    "\n",
    "diabetes데이터셋은 test 데이터셋을 갖고 있지 않습니다. 기존하는 입력 데이터 x의 평균값(average)과 중간값(median)을 구합니다. 이것을 두 개의 입력 데이터로 간주하여 앞에서 구한 모델을 사용하여 각각의 예측 값을 구하여 그래프에 각각 표시하십시오. \n",
    "\n",
    "\n",
    "<img src=\"https://github.com/idebtor/KMOOC-ML/blob/master/ipynb/images/ai4all-diabetes9.png?raw=true\" width=\"400\">\n",
    "<center>그림 1: 입력 데이터 평균값과 중간값에 대하여 모델을 사용하여 결과 예측하기 </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How to:\n",
    "- average_x 구하기: sum() and len() 사용합니다. \n",
    "- median_x 구하기: np.sort() and // 를 사용합니다. \n",
    "- average_pred 구하기: 앞에서 구한 모델 y_pred = wx + b를 사용합니다. \n",
    "- median_pred 구하기: \n",
    "- graph 그리기\n",
    "\n",
    "#### Solution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_diabetes\n",
    "diabetes = load_diabetes()\n",
    "x = diabetes.data[:, 2]     # bmi feature 읽어오기 \n",
    "y = diabetes.target         # target/label 읽어오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_x = sum(x) / len(x)\n",
    "average_pred = average_x * w + b\n",
    "print(average_pred)\n",
    "\n",
    "sorted_x = np.sort(x)\n",
    "median_x = sorted_x[len(sorted_x) // 2]\n",
    "median_pred = median_x * w + b\n",
    "print(median_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.scatter(x, y, marker = '.', color = 'g')\n",
    "plt.scatter(average_x, average_pred, color = 'r')\n",
    "plt.scatter(median_x, median_pred, color = 'b')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 에폭$^{epoch}$에 따른 오차의 변화를 관찰하기\n",
    "\n",
    "일반적으로 경사 하강법에서는 주어진 훈련 데이터로 학습을 여러 번 반복합니다. 훈련 데이터셋 전체를 이용하여 한 단위의 작업을 진행하는 것을 epoch(에폭)이라고 부릅니다. 기계 학습에서 에폭을 반복함으로 오차를 줄이는 것은 흔히 있는 일입니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "한 번의 에폭에서 각 샘플에 따라 오차는 양수가 되기도 하고, 음수가 되기도 하기 때문에, 이런 오차들을 단순히 모두 합하여 산출하는 것은 모든 샘플에 대한 오차를 제대로 대표할 수 없습니다. 이를 방지하기 위하여 오차에 제곱한 값을 모두 더 하는 방법을 사용합니다. 이를 __SSE__ 즉 Sum of Squared Error라고 하며, 이 전체 값을 샘플의 수로 나눈 것을 __MSE__ 즉 Mean Squared Error(평균 제곱 오차)라고 하며, 다음과 같이 계산할 수 있습니다. MSE 혹은 SSE는 기계학습에서 가장 일반적이고 직관적인 오차의 지표로 사용하고 있으며, 이러한 오차의 값들은 낮을수록 좋습니다. \n",
    "\n",
    "\\begin{align}\n",
    "SSE &= \\sum^n_{i=1} (y_i - \\hat{y}_i)^2  \\tag{1} \\\\\n",
    "MSE &= \\frac{1}{n}\\sum^n_{i=1} (y_i - \\hat{y}_i)^2  \\tag{2}\n",
    "\\end{align}\n",
    "\n",
    "여기서 $i=1$ 샘플을 의미하며, $y_i$는 레이블(타깃)값이며, $y_i$는 예측값입니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위의 Diabetes 예제에서 한번의 에폭의 계산에서 산출한 MSE를 에폭의 횟수만큼 축적하여 다음과 같이 시각화하는 코드를 완성하십시오.   \n",
    "\n",
    "<img src=\"https://github.com/idebtor/KMOOC-ML/blob/master/ipynb/images/ai4all-diabetes11.png?raw=true\" width=\"400\">\n",
    "<center>그림 2: 경사하강법 오차(MSE)의 변화 </center>\n",
    "\n",
    "#### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_diabetes\n",
    "diabetes = load_diabetes()\n",
    "\n",
    "x = diabetes.data[:, 2]     # bmi feature 읽어오기 \n",
    "y = diabetes.target         # target/label 읽어오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "w = 1.0\n",
    "b = 1.0\n",
    "sse = np.zeros(epochs)                # initialize sse array as 0\n",
    "mse = np.zeros(epochs)\n",
    "for i in range(epochs):\n",
    "    sse_i = 0                         # compute sse per epoch \n",
    "    for x_i, y_i in zip(x, y):\n",
    "        y_hat = x_i * w + b\n",
    "        error = y_i - y_hat\n",
    "        w_rate = x_i\n",
    "        w = w + w_rate * error\n",
    "        b = b + 1 * error\n",
    "        sse_i += (error**2)        # accumulate error for each sample\n",
    "    sse[i] = sse_i                   # get sse per epoch\n",
    "    mse[i] = sse[i] / len(x)                     # get mse from sse\n",
    "print(w, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "x0 = np.arange(epochs)\n",
    "plt.plot(x0, mse)            \n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('mse')\n",
    "plt.title('Gradient Descent MSE for Diabetes Dataset')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FYI: For your information\n",
    "### 선형 회귀 모델에 사용한 그래프를 그리는 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_diabetes\n",
    "diabetes = load_diabetes()\n",
    "\n",
    "x = diabetes.data[:, 2]\n",
    "y = diabetes.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot([-0.1, 0.18], [125, 330], 'r')\n",
    "plt.scatter(diabetes.data[:, 2], diabetes.target, marker='^', color='green')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot([-0.1, 0.18], [30, 330], 'r')\n",
    "plt.scatter(diabetes.data[:, 2], diabetes.target, marker='^', color='green')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot([-0.1, 0.18], [330, 25], 'r')\n",
    "plt.scatter(diabetes.data[:, 2], diabetes.target, marker='^', color='green')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 참고문헌\n",
    "\n",
    "1. 케라스 창시자에게 배우는 딥러닝, 프랑소와 숄레, 길벗\n",
    "1. 핸즈온 머신러닝, 오렐리앙 제롱, 한빛미디어\n",
    "1. 딥러닝 입문, 박해선, 이지스 퍼블리싱\n",
    "1. 파이썬으로 배우는 기계학습, 김영섭, K-MOOC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------\n",
    "__Be joyful always!__ 1 Thes.5:16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그러나 너희는 택하신 족속이요 왕 같은 제사장들이요 거룩한 나라요 그의 소유가 된 백성이니 이는 너희를 어두운 데서 불러 내어 그의 기이한 빛에 들어가게 하신 이의 아름다운 덕을 선포하게 하려 하심이라 (벧전2:9)\n",
    "\n",
    "-------\n",
    "\n",
    "# Welcome to \"AI for All\"\n",
    "\n",
    "Lecture Notes by idebtor@gmail.com, Handong Global University"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Chapter 4.  손실함수와 경사하강법(Gradient Descent)\n",
    "\n",
    ":본 단원은 참고문헌 (3)에서 발췌한 것입니다. \n",
    "\n",
    "--------------\n",
    "경사 하강법에 사용한 손실 함수가 어떻게 사용된 것인지 알아봅니다. 경사 하강법은 정의된 손실 함수의 값이 최소가 되는 지점을 찾아가는 방법입니다. 여기서 손실 함수란 예측 값과 타깃 값의 차이를 함수로 정의한 것을 말합니다. 지금까지 우리가 사용한 방법인 '오차를 변화율에 곱하여 가중치와 편향 조정하기'는 제곱 오차라는 손실 함수를 미분한 것과 같습니다. 여기서는 '손실 함수'으로부터 경사 하강법을 접근하는 방법을 시도합니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 손실 함수의 모든 것\n",
    "\n",
    "제곱 오차(Squared Error)는 타깃(실제)값과 예측값을 뺀 다음 제곱한 것입니다. 제곱 오차를 수식으로 나타내면 다음과 같습니다. \n",
    "\n",
    "\\begin{align}\n",
    "SE = (y - \\hat{y})^2  \\tag{1}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__이때 제곱 오차가 최소가 되면 산점도 그래프를 가장 잘 표현한 직선이 그려집니다.__ 즉, 제곱 오차의 최솟값을 찾는 방법을 알면 모델을 쉽게 만들 수 있습니다. 다음은 제곱 오차와 직선의 관계를 나타낸 그래프입니다. \n",
    "\n",
    "<img src=\"https://github.com/idebtor/KMOOC-ML/blob/master/ipynb/images/ai4all-diabetes8.png?raw=true\" width=\"400\">\n",
    "<center>그림 1: 제곱 오차(SE)와 선형 회귀(직선)의 관계 </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__제곱 오차 함수의 최솟값을 알아내려면 기울기에 따라 함수의 값이 낮은 쪽으로 이동해야 합니다.__ 기울기를 구하려면 제곱 오차를 가중치나 편향에 대해 미분하면 됩니다.  다행히 제곱 오차는 가중치나 편향에 대해 미분이 가능합니다. 그러면 먼저 가중치에 대하여 제곱 오차를 미분해 보겠습니다. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 가중치에 대하여 제곱 오차 미분하기\n",
    "\n",
    "다음 식은 제곱 오차를 가중치($w$)에 대하여 편미분한 것입니다. \n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial{SE}}{\\partial{w}}  \n",
    "&= \\frac{\\partial}{\\partial{w}} (y - \\hat{y})^2   \\qquad{\\because{ f(g(x))' = f'(g(x))g'(x) } } \\tag{2} \\\\  \n",
    "&= 2(y - \\hat{y}) \\frac{\\partial}{\\partial{w}} (y - \\hat{y}) \\qquad{\\because{\\text{y is a constant}} } \\\\\n",
    "&= 2(y - \\hat{y}) (- \\frac{\\partial}{\\partial{w}} \\hat{y}) \\\\\n",
    "&= 2(y - \\hat{y}) (- \\frac{\\partial}{\\partial{w}} (wx + b)) \\qquad{\\because{\\text{x, b are constants}} } \\\\\n",
    "&= 2(y - \\hat{y}) (- x) \\\\\n",
    "&= -2(y - \\hat{y}) x \\tag{3} \\\\\n",
    "\\end{align}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 편미분이란 미분의 대상 변수($w$)를 제외한 다른 변수($x, b$ 등)를 상수로 취급하여 미분한 것입니다. 이 때 $y$는 준비된 타깃 데이터이므로 $w$의 함수가 아니고 $\\hat{y}$은 $w$의 함수($\\hat{y} = wx + b$)입니다. \n",
    "\n",
    "따라서 $\\hat{y}$을 $w$에 대해 미분하면 상수항 $b$는 사라지고 $x$만 남습니다. 결국 제곱 오차를 가중치($w$)에 대해 미분한 결과는 다음과 같습니다. \n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial{SE}}{\\partial{w}} = -2(y - \\hat{y}) x \\tag{4}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "최기에 제곱 오차 공식을 $(y - \\hat{y})^2$가 아니라 $\\frac{1}{2}(y - \\hat{y})^2$로 정의했다면 2와 $\\frac{1}{2}$이 서로 약분되어 조금 더 깔끔하게 $-(y-\\hat{y})x$ 로 표현할 수 있었을 것입니다. 그래서, 보통은 제곱 오차 공식을 2로 나눈 함수를 편미분합니다. 여기에서도 아래와 같이 가중치에 대한 __제곱 오차의 변화율__은 $-(y-\\hat{y})x$ 를 사용하도록 하겠습니다. \n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial{SE}}{\\partial{w}} = -(y - \\hat{y}) x \\tag{5}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 미분한 오차 함수로 가중치 조정하기\n",
    "\n",
    "가중치에 대한 제곱 오차의 변화율을 구했습니다. 이제 기존의 가중치를 조정하기 위하여, 앞에서 가중치를 조정할 때 변화율을 더했던 것과 마찬가지로 가중치를 조정합니다. 여기서는 __$w$에서 변화율을 뺍니다. $w$에서 변화율을 더하지 않고 빼는 이유는 손실 함수의 낮은 쪽으로 이동하고 싶기 때문입니다__. \n",
    "\n",
    "\n",
    "\\begin{align}\n",
    "w_{new} &= w - \\frac{\\partial{SE}}{\\partial{w}}  \\\\\n",
    "        &= w + (y - \\hat{y}) x \\tag{6}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위의 식을 어디서 본 것 같지 않나요? \n",
    "\n",
    "앞에서 오차 역전파를 알아보면 작성한 코드에 이와 같은 식이 있었습니다. 오차 역전파를 알아보며 적용했던 수식(`w + w_rate * error`) 혹은 (`w + x_i * error`) 은 사실 제곱 오차를 미분했던 것과 같았던 것입니다. \n",
    "\n",
    "아래 코드는 오차 역전파를 사용하여 가중치 w를 조정하는 코드입니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "y_hat = x_i * w + b  \n",
    "error = y_i - y_hat  \n",
    "\n",
    "\n",
    "w = w + x_i * error                 # w_rate = x_i  \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "실제 코딩에서, 반복문을 사용하여 여러 샘플에 적용해야 하므로, x 대신에 x_i, w_new 대신 w로 표기하였습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 편향에 대하여 제곱 오차 미분하기\n",
    "\n",
    "편향에 대하여 제곱 오차를 미분할 때는 처음부터 $\\frac{1}{2}$를 곱한 제곱 오차 공식을 사용합니다. 유도식은 다음과 같습니다. \n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial{SE}}{\\partial{b}}  \n",
    "&= \\frac{\\partial}{\\partial{b}} \\frac{1}{2}(y - \\hat{y})^2   \\qquad{\\because{ f(g(x))' = f'(g(x))g'(x) } } \\tag{7} \\\\\n",
    "&= (y - \\hat{y}) \\frac{\\partial}{\\partial{b}} (y - \\hat{y}) \\qquad{\\because{\\text{y is a constant}} } \\\\\n",
    "&= (y - \\hat{y}) (- \\frac{\\partial}{\\partial{b}} \\hat{y}) \\\\\n",
    "&= (y - \\hat{y}) (- \\frac{\\partial}{\\partial{b}} (wx + b)) \\qquad{\\because{\\text{w, x are constants}} } \\\\\n",
    "&= (y - \\hat{y}) (-1) \\\\\n",
    "&= -(y - \\hat{y}) 1 \\tag{8} \\\\\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "미분 과정은 앞에 했던 $w$에 대한 편미분 과정과 매우 비슷합니다. 다만, 여기는 $w$, $x$가 상수가 됩니다. 가중치에서 변화율을 뺐던 이유와 같은 이유로 편향에서 변화율을 뺍니다. \n",
    "\n",
    "\\begin{align}\n",
    "b_{new} &= b - \\frac{\\partial{SE}}{\\partial{b}}  \\\\\n",
    "        &= b + (y - \\hat{y})  \\tag{9}\n",
    "\\end{align}\n",
    "\n",
    "이 식도 앞에서 작성한 코드와 정확히 일치합니다. 다음 코드를 참고하십시오. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "error = y_i - y_hat\n",
    "b = b + 1 * error\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "앞으로는 손실 함수에 대해 일일이 변화율의 값을 계산하는 대신 편미분을 사용하여 변화율을 계산합니다. 그리고 변화율은 인공지능 분야에서 특별히 Gradient(그레디언트, 경사)라고 부릅니다. 앞으로는 변화율이란 용어 대신 __Gradient__를 사용합니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "지금까지 경사 하강법을 사용한 회귀 문제를 변화율을 직접 구하는 방식과 미분을 사용한 방식(손실 함수)으로 검증해 보았습니다. 다음에는 지금까지 배운 내용을 모두 정리하여 하나의 파이썬 클래스를 만들어 봅니다.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise \n",
    "\n",
    "### 1. 다른 방법으로 가중치($w$)에 대하여 제곱 오차 미분하기\n",
    "\n",
    "위에서 제곱 오차($SE$)를 가중치($w$)에 대하여 편미분할 때, 미분 공식 $f(g(x))' = f'(g(x))g'(x)$을 처음에 사용하였습니다. 여기서 연습 문제에서는 $(y - \\hat{y})^2$을 풀어서 나온 $(y - 2y\\hat{y}  + \\hat{y}^2)$ 식부터 식(3)구하는 편미분 과정을 아래에 반복하십시오. 마크 다운을 사용하여 구체적으로 미분 과정을 작성하십시오. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "\\frac{\\partial{SE}}{\\partial{w}}  \n",
    "&= \\frac{\\partial}{\\partial{w}} (y - \\hat{y})^2    \\\\  \n",
    "&= \\frac{\\partial}{\\partial{w}}(y^2 - 2y\\hat{y}  + \\hat{y}^2) \\qquad{\\because{\\text{y is a constant}} } \\\\\n",
    "&= -2y + \\frac{\\partial}{\\partial{w}}(\\hat{y}^2) \\\\\n",
    "... \\\\\n",
    "&= -2(y - \\hat{y}) x \n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 다른 방법으로 편향($b$)에 대하여 제곱 오차 미분하기\n",
    "\n",
    "위에서 제곱 오차($SE$)를 편향($b$)에 대하여 편미분할 때, 미분 공식 $f(g(x))' = f'(g(x))g'(x)$을 처음에 사용하였습니다. 여기서 연습 문제에서는 $(y - \\hat{y})^2$을 풀어서 나온 $(y - 2y\\hat{y}  + \\hat{y}^2)$ 식부터 위의 식(8)구하는 편미분 과정을 아래에 반복하십시오. 마크 다운을 사용하여 구체적으로 미분 과정을 작성하십시오. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "\\frac{\\partial{SE}}{\\partial{b}}  \n",
    "&= \\frac{\\partial}{\\partial{b}} \\frac{1}{2}(y - \\hat{y})^2   \\\\\n",
    "&= \\frac{\\partial}{\\partial{b}}\\frac{1}{2}(y - 2y\\hat{y}  + \\hat{y}^2) \\qquad{\\because{\\text{y is a constant}} } \\\\\n",
    "&= -y + \\frac{\\partial}{\\partial{b}}\\frac{1}{2}(\\hat{y}^2) \\\\\n",
    "&= -(y - \\hat{y}) 1  \\\\\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 참고문헌\n",
    "\n",
    "1. 케라스 창시자에게 배우는 딥러닝, 프랑소와 숄레, 길벗\n",
    "1. 핸즈온 머신러닝, 오렐리앙 제롱, 한빛미디어\n",
    "1. 딥러닝 입문, 박해선, 이지스 퍼블리싱\n",
    "1. 파이썬으로 배우는 기계학습, 김영섭, K-MOOC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------\n",
    "__Be joyful always!__ 1 Thes.5:16"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
