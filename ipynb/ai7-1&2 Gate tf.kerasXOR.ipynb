{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "우리가 알거니와 하나님을 사랑하는 자 곧 그의 뜻대로 부르심을 입은 자들에게는 모든 것이 합력하여 선을 이루느니라. 롬8:28  \n",
    "\n",
    "And we know that in all things God works for the good of those who love him, who have been called according to his purpose. (Rom 8:28)\n",
    "\n",
    "-------\n",
    "\n",
    "# Welcome to \"AI for All\"\n",
    "\n",
    "Lecture Notes by idebtor@gmail.com, Handong Global University"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 제 1 강 신경망을 내 손으로 만져보기(Gate)\n",
    "\n",
    "---------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 1. AND 연산 \n",
    "\n",
    "AND 연산은 모두 입력이 참일 때, 참을 결과로 출력하는 연산입니다.  다음의 진리표에 표시된 바와 같습니다. \n",
    "\n",
    "<img src=\"https://github.com/idebtor/KMOOC-ML/blob/master/ipynb/images/truthtable.png?raw=true\" width=\"400\">\n",
    "<center>그림 1:  OR, AND, NAND, XOR 진리표 </center>\n",
    "\n",
    "\n",
    "이를 넘파이 배열로 나타내면 다음과 같습니다.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "x = np.array(None) \n",
    "y = np.array(None)\n",
    "print(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "기계 학습의 관점으로 AND 로직 연산을 바라보면, 입력은 두 개 특성($x_1, x_2$)을 가지고 있으며, 네 개의 샘플이 있으며, 따라서 네 개의 각 샘플에 대한 레이블(혹은 타깃)이 있습니다. 이러한 샘플들과 레이블이 곧 퍼셉트론의 훈련 데이터셋이 됩니다. \n",
    "\n",
    "우리는 사실상 위에서 훈련 데이터셋을 준비한 것입니다. 테스트 혹은 예측을 위한 데이터셋은 존재하지 않으므로, 입력 데이터셋을 그대로 테스트를 위해서도 사용할 것입니다. \n",
    "\n",
    "다음은 데이터셋의 샘플 갯수와 레이블 갯수가 같은지 점검하고, 샘플 갯수와 에폭을 설정합니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(x) == len(y)\n",
    "samples = None\n",
    "epochs = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "모델을 학습시키기 전에, 가중치와 편향도 정규 분포를 가지는 난수로 초기화 해줍니다. 또한, 학습률(eta)은 0.1로 설정하였습니다.  입력값(특성)이 두개이므로, 가중치도 각각에 맞춰서 두개를 선언해줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = np.array(None)                 # w1, w2\n",
    "b = np.array(None)                 # b\n",
    "eta = 0.1                          # 학습률(learning rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 학습을 시켜보겠습니다. 앞에서 만든 편향을 가진 뉴런처럼 코드를 구성하면 됩니다. 각각 네가지 경우를 한번씩 학습할 때마다, 네가지 를 예측한 값과 실제 값의 차이로 계산할 수 있는 SSE(sum of squared error)를 구해서 살펴보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def activation(x):  \n",
    "    return None         # sigmoid function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(epochs): \n",
    "    sse_i = 0 \n",
    "    for j in range(samples): \n",
    "        z = None       # net input\n",
    "        a = None               # apply activation function\n",
    "        error = None             # error\n",
    "        w += None      # new weight\n",
    "        b += None            # new bias\n",
    "        sse_i += None\n",
    "    if (i + 1) % 100 == 0: \n",
    "        print(i + 1, sse_i / 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이러한 학습을 통해서 결정된 것은 무엇입니까? \n",
    "\n",
    "모델의 가중치와 편향입니다.  \n",
    "\n",
    "이제 학습을 통해서 얻은 기계 학습 모델에 새로운 데이터를 제공하여 출력을 예측하고자 합니다. 다만, 여기서는 예측에 사용할 데이터가 없으므로, 학습에 이미 사용한 데이터를 사용하여 각각의 케이스를 예측하는 것이 합당합니다. 이를 계산하는 방법은 다음과 같이 퍼셉트론의 __순입력(z)__를 구하고 이를 __활성화 함수에 적용하면__ 예측값을 계산할 수 있습니다. __즉 모델의 순뱡향(forpass)만 계산하면 예측값을 산출할 수 있습니다.__ 이러한 두 연산을 합하여 forpass() 메소드로 구현합니다. \n",
    "```\n",
    "z = np.sum(x[i] * w) + b \n",
    "a = activation(z)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(samples): \n",
    "    print('{} {} y_hat:{}'.format(x[i], y[i], None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위와 같이, 실제로 1이 나와야하는 값은 1에 가깝게, 0에 가까워야하는 값은 0에 가깝게 예측이 되었습니다. `[0 0]` 케이스의 경우에는 다른 케이스들 보다 더 0에 가까운걸로 보아 더 확실한 케이스라는 점을 알 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. OR 연산\n",
    "\n",
    "OR 연산은 입력들 중에 하나라도 참이면, 참을 결과로 출력하는 연산입니다.  이를 넘파이 배열로 나타내면 다음과 같습니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([[1, 1], [1, 0], [0, 1], [0, 0]], \"float32\") \n",
    "y = np.array([[1], [1], [1], [0]],  \"float32\")\n",
    "\n",
    "w = np.array([1.0, 1.0])            # w1, w2\n",
    "b = np.array([1.0])                 # b \n",
    "eta = 0.1                           # 학습률(learning rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이를 AND와 같은 가중치, 편향, 학습률 및 학습 모델로 학습을 시키면 다음과 같습니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "학습을 진행할 수록 error값의 합이 0에 가까워지는것을 확인할 수 있습니다. 각각의 케이스들을 해당 가중치와 편향으로 예측해보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. XOR 연산\n",
    "XOR은 AND나 OR연산과는 다르게 홀수 개의 입력이 참일 때, 결과가 참이 됩니다. 입력을 두개라고 한다면, 위의 진리표에 나타난 바와 같습니다.\n",
    "\n",
    "이를 넘파이 배열로 나타내면 다음과 같습니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([[1, 1], [1, 0], [0, 1], [0, 0]],  \"float32\") \n",
    "y = np.array([[0], [1], [1], [0]],  \"float32\")\n",
    "\n",
    "w = np.array([1.0, 1.0])            # w = tf.random.normal([2], 0, 1)         # mean = 0, std dev = 1\n",
    "b = np.array([1.0])                 # b = tf.random.normal([1], 0, 1) \n",
    "eta = 0.1                           # 학습률(learning rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이를 AND와 같은 가중치, 편향, 학습률 및 학습 모델로 학습을 시키면 다음과 같습니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(epochs): \n",
    "    sse_i = 0 \n",
    "    for j in range(samples): \n",
    "        z = np.sum(x[j] * w) + b        # net input\n",
    "        a = activation(z)               # apply activation function\n",
    "        error = y[j][0] - a             # error\n",
    "        w = w + eta * error * x[j]      # new weight\n",
    "        b = b + eta * error             # new bias\n",
    "        sse_i += error * error\n",
    "    if (i + 1) % 100 == 0: \n",
    "        print(i + 1, sse_i / 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위와 같이, 특정 학습 지점부터 에러 값에 변화가 거의 없습니다. 이것이 과연 학습이 잘된걸까요? 계산된 가중치와 편향을 가지고 각 케이스를 계산해보았습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(samples): \n",
    "    print('{} {} y_hat:{}'.format(x[i], y[i], activation(np.sum(x[i] * w) + b)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위와 같이, 결과가 0 혹은 1에 가까운 값이 나와 하지만, 모든 케이스가 0.5에 가깝게 나오고 있습니다. 이것은 원하던 결과가 아닙니다. 결과를 해석해보자면, 가중치와 편향 값은 모두 케이스 순서에 의존적이 된다는 것을 알 수 있습니다. 먼저 들어간 `[1 1]`이라는 케이스가 네번째에 들어가는 `[0 0]`이라는 케이스보다 영향을 준다는 것입니다. `[1 1]`이라는 케이스가 먼저 들어가서 가중치와 편향에 중대한 영향을 미치고 이 값들을 가지고 학습을 진행한다는 것이 문제입니다.\n",
    "\n",
    "그러면 XOR문제는 풀지 못하는 걸까요? 여러 층의 퍼셉트론을 사용하면 해결이 됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 단층 퍼셉트론에서의 XOR 문제점\n",
    "\n",
    "인공 신경망에서는 단층 퍼셉트론으로 XOR 연산이 불가능하다는 것은 마빈 민스키 등에 의해서 밝혀졌습니다. 이러한 내용이 밝혀지면서 인공지능의 겨울이 찾아왔었습니다. 그야말로 전설같은 이야기로 잘 알려져 있습니다. \n",
    "\n",
    "<img src=\"https://github.com/idebtor/KMOOC-ML/blob/master/ipynb/images/ai4all-history.jpg?raw=true\" width=\"900\">\n",
    "<center>그림 2: XOR 문제와 인공 지능의 발전사</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "인공 신경망(ANN, Aritificial Neural Networks)은 1943년 신경생리학자 Warren McCulloch과 수학자 Walter Pitts가 'A Logical Calculus of Ideas Immanent In Nervous Activity' 처음 소개했습니다. 그 이후 1960년대까지 당싱에 등장한 인공 신경망을 통해 사람들은 지능을 가지 기계가 상당히 엄청난 일을 해낼 것이라 생각했습니다. 그러나, 위의 그림(출처: [beamandrew's blog](https://beamandrew.github.io/deeplearning/2017/02/23/deep_learning_101_part1.html))처럼 사람들의 기대와는 달리 인공 신경망으로 XOR문제를 해결할 수 없게 되었고, 인공 지능과 관련한 연구는 암흑기로 접어 들게 되었다. 그래도, 1990년 대에는 SVM과 성능이 좋은 다른 머신러닝 알고리즘들이 나올 정도도 꾸준한 연구가 진행은 되고 있었습니다. \n",
    "\n",
    "2000년대에 들어서면서 인공 신경망은 2012년 ILSVRC2012 대회에서 인공 신경망을 깊게 쌓은 딥러닝 모델인 AlexNet이 압도적인 성적으로 우승하면서 다시금 주목받게 되었습니다. 이렇게 인공 신경망에 기반을 둔 딥러닝이 다시 주목받게 된 계기가 되었습니다. \n",
    "\n",
    "인공 지능의 발전사에서 약간의 의미가 있었던 XOR 문제를 아래 그림과 같은 다층 신경망을 통해 이제 어렵지 않게 풀어볼 수 있는 문제가 되었습니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://github.com/idebtor/KMOOC-ML/blob/master/ipynb/images/ai4all-xor.png?raw=true\" width=\"600\">\n",
    "<center>그림 3:  XOR 연산을 위한 다층 인공 신경망</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 제 2 강 신경망을 내 손으로 만져보기(tf.keras & XOR)\n",
    "\n",
    "---------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2000년대에 들어서면서 인공 신경망은 2012년 ILSVRC2012 대회에서 인공 신경망을 깊게 쌓은 딥러닝 모델인 AlexNet이 압도적인 성적으로 우승하면서 다시금 주목받게 되었습니다. 이렇게 인공 신경망에 기반을 둔 딥러닝이 다시 주목받게 된 계기가 되었습니다. \n",
    "\n",
    "인공 지능의 발전사에서 약간의 의미가 있었던 XOR 문제를 이제 어렵지 않게 풀어볼 수 있는 문제가 되었습니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. XOR 연산 다층 신경망\n",
    "\n",
    "\n",
    "<img src=\"https://github.com/idebtor/KMOOC-ML/blob/master/ipynb/images/ai4all-xor.png?raw=true\" width=\"500\">\n",
    "<center>그림 2:  XOR 연산을 위한 다층 인공 신경망</center>\n",
    "\n",
    "XOR 연산을 위한 2단의 Dense Layer로 구성하였습니다. Dense는 기본적인 레이어로, 입력과 출력 사이에 있는 모든 뉴런이 서로 연결되어 있는 레이어입니다. Dense Layer는 아래와 같이 선언이 가능합니다.\n",
    "\n",
    "```\n",
    "tf.keras.layers.Dense()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "각각의 Layer는 순차적으로 배치되어있습니다. 이를 Sequential 신경망이라고 합니다. 이것은 아래와 같이 선언합니다.\n",
    "```\n",
    "tf.keras.Sequential()\n",
    "```\n",
    "Sequential() Dense Layer를 2층으로 쌓도록 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "model = tf.keras.Sequential([ \n",
    "    tf.keras.layers.Dense(2, activation='sigmoid', input_shape=(2,)), \n",
    "    tf.keras.layers.Dense(1, activation='sigmoid') \n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "순차적인 신경망안에 두개의 Dense Layer를 배치하고, 첫번째 Dense Layer는 2개의 뉴런을 선언하였고, 각 뉴런은 sigmoid를 활성함수로 가집니다. 입력 값은 두개이므로 모양이 (2,) 모양입니다. 두번째 Dense Layer는 1개의 뉴런을 선언하고, 마찬가지로 sigmoid 활성함수를 사용하였습니다.\n",
    "\n",
    "XOR연산이므로, 입력 x와 출력(레이블, 타깃) y값은 다음과 같습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "x = np.array([[1, 1], [1, 0], [0, 1], [0, 0]]) \n",
    "y = np.array([[0], [1], [1], [0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제는 model을 준비시키는 명령어를 사용하여 최적화 함수(optimizer)와 손실 함수(loss)를 정의합니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer = tf.keras.optimizers.SGD(learning_rate=0.1), # 'adam', 'sgd', ...\n",
    "    loss = 'mse',                                           # 'mean_squared_error' 'binary_crossentropy'\n",
    "    metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tf.keras에서는 다양한 최적화 함수와 손실 함수를 제공합니다. 그 중에서도 SGD(Stochastic Gradient Descent)는 확률적 경사 하강법이라고 합니다. 경사 하강법은 앞선 포스팅에서 설명하였고, 이 경사 하강법을 한번에 계산(전체 데이터세트를 사용)하지 않고 확률을 이용하여 부분적으로 나눠서 계산을 한다는 의미입니다.\n",
    "\n",
    "SGD(Stochastic Gradient Descent)\n",
    "\n",
    "- 기존의 경사 하강법은 데이터세트의 크기가 너무 커지면 계산이 오래 걸리는 단점이 있었습니다.\n",
    "- SGD는 반복당 하나의 데이터(Batch=1)만을 사용하여 적은 계산으로 기울기를 얻어내는 방식입니다.\n",
    "- 단점: 반복이 충분하면 효과는 좋지만, 노이즈가 심합니다. 최저점을 찾는다는 보장이 없습니다. 가능성만 높을 뿐입니다.\n",
    "- 위의 단점을 극복하기 위해서 미니 배치 SGD가 있습니다. 배치를 너무 크게도 너무 작게도 잡지 않고 SGD보다 노이즈는 적게, 일반 경사 하강법보다는 효율적으로 찾는 방식입니다. \n",
    "\n",
    "손실 함수(Loss Function)은 RMSE의 제곱인 MSE를 사용하기로 합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아래와 같이, summary() 메소드를 사용하여 형성된 신경망의 구조를 살펴볼 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "파라미터는 첫번째 Layer에서 입력값에 가중치가 곱해지고 편향이 더해져서 은닉층의 각 노드에 3개의 파라미터가 전달되어 총 6개입니다. 두번째 Layer에서는 첫 Layer에서 나온 출력 값 두개와 편향을 합쳐서 총 3개의 값이 두번째 층에 전달되어 결과적으로 하나의 출력(예측) 값이 나오게 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "history = model.fit(x, y, epochs = 5000, batch_size = 1, verbose = 0)\n",
    "print(model.evaluate(x, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "학습을 진행하면 위와 같은 출력이 나오며, 손실 값을 알려줍니다. 손실 값은 학습을 진행할수록 0에 가까워지는 것을 확인할 수 있습니다.\n",
    "\n",
    "이제 제대로 학습이 되었는지 확인해봅니다. 아래의 코드를 통해서 x에 대한 예측 값을 출력 받을 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(model.predict(x))\n",
    "print(model.predict_classes(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 다음과 같은 코드를 사용하면 경고(predict_classes) 메시지를 피할 수 있습니다. \n",
    "print(model.predict(x))    \n",
    "print((model.predict(x) > 0.5).astype(\"int32\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "정답은 0, 1, 1, 0 순서입니다. 각각 0과 1에 해당하는 수에 가깝게 나오고 있는것을 확인 할 수 있습니다. 이는 학습을 더 많이 할수록 더 정답에 해당하는 수치로 갈것 입니다.\n",
    "\n",
    "이제는 가중치와 편향 값을 확인해봅니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for weight in model.weights: \n",
    "    print(weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(history.history['loss'])\n",
    "plt.title('Model Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['accuracy'])\n",
    "plt.title('Model Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Model Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Tweaking the model.\n",
    "- increasing the number of hidden layers or nodes \n",
    "- trying to use different activation functions or optimizer\n",
    "- using different batch sizes, 1 or 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([ \n",
    "    tf.keras.layers.Dense(32, activation='relu', input_shape=(2,)), \n",
    "    tf.keras.layers.Dense(32, activation='relu'), \n",
    "    tf.keras.layers.Dense(1, activation='sigmoid') \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([ \n",
    "    tf.keras.layers.Dense(64, activation='relu', input_shape=(2,)), \n",
    "    tf.keras.layers.Dense(1, activation='sigmoid') \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='mean_squared_error',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question: \n",
    "\n",
    "Is it possible to solve XOR with no activation function at all, using more neurons or layers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([ \n",
    "    tf.keras.layers.Dense(64, activation='linear', input_shape=(2,)), \n",
    "    tf.keras.layers.Dense(1, activation='sigmoid') \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear(x):\n",
    "    '''\n",
    "    The function returns the variable that is passed in, so all types work.\n",
    "    '''\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------\n",
    "__Be joyful always!__ 1 Thes.5:16"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
